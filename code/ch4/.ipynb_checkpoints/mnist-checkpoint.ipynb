{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 텐서플로우 저장소에서 데이터를 다운로드 받습니다.\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data(path='mnist.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터의 형태 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,)\n",
      "[5 0 4 ... 5 6 8]\n",
      "(10000, 28, 28) (10000,)\n",
      "[7 2 1 ... 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "# 훈련 데이터\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(y_train)\n",
    "\n",
    "# 테스트 데이터\n",
    "print(x_test.shape, y_test.shape)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 그려보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.random.seed(777)\n",
    "\n",
    "sample_size = 3\n",
    "# 0~59999의 범위에서 무작위로 3개의 정수를 뽑습니다.\n",
    "random_idx = np.random.randint(60000, size=sample_size) \n",
    "\n",
    "for idx in random_idx:\n",
    "    img = x_train[idx, :]\n",
    "    label = y_train[idx]\n",
    "    plt.figure()\n",
    "    plt.imshow(img)\n",
    "    plt.title('%d-th data, label is %d' % (idx,label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 검증 데이터 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터 (42000, 28, 28) 레이블 (42000,)\n",
      "검증 데이터 (18000, 28, 28) 레이블 (18000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 훈련/테스트 데이터를 0.7/0.3의 비율로 분리합니다.\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = 0.3, random_state = 777)\n",
    "print('훈련 데이터 {} 레이블 {}'.format(str(x_train.shape), str(y_train.shape)))\n",
    "print('검증 데이터 {} 레이블 {}'.format(str(x_val.shape), str(y_val.shape)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 입력을 위한 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 784)\n"
     ]
    }
   ],
   "source": [
    "num_x_train = x_train.shape[0]\n",
    "num_x_val = x_val.shape[0]\n",
    "num_x_test = x_test.shape[0]\n",
    "\n",
    "# 모델의 입력으로 사용하기 위한 전처리 과정입니다.\n",
    "x_train = (x_train.reshape((num_x_train, 28 * 28))) / 255\n",
    "x_val = (x_val.reshape((num_x_val, 28 * 28))) / 255\n",
    "x_test = (x_test.reshape((num_x_test, 28 * 28))) / 255\n",
    "\n",
    "print(x_train.shape) # 모델 입력을 위해 데이터를 784차원으로 변경합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 입력을 위한 레이블 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# 각 데이터의 레이블을 범주형 형태로 변경합니다.\n",
    "y_train = to_categorical(y_train)\n",
    "y_val = to_categorical(y_val)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "# 입력 데이터의 형태를 꼭 명시해야 합니다.\n",
    "# 784차원의 데이터를 입력으로 받고, 64개의 출력을 가지는 첫 번째 Dense 층\n",
    "model.add(Dense(64, activation = 'relu', input_shape = (784, )))\n",
    "model.add(Dense(32, activation = 'relu')) # 32개의 출력을 가지는 Dense 층\n",
    "model.add(Dense(10, activation = 'softmax')) # 10개의 출력을 가지는 신경망"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 소프트맥스와 시그모이드 값의 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 소프트맥스 함수\n",
    "def softmax(arr):\n",
    "    m = np.max(arr)\n",
    "    arr = arr - m # exp의 오버플로우 방지\n",
    "    arr = np.exp(arr)\n",
    "    return arr / np.sum(arr)\n",
    "\n",
    "# 시그모이드 함수\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "case_1 = np.array([3.1, 3.0, 2.9])\n",
    "case_2 = np.array([2.0, 1.0, 0.7])\n",
    "\n",
    "np.set_printoptions(precision=3) # numpy 소수점 제한\n",
    "print('sigmoid {}, softmax {}'.format(sigmoid(case_1), softmax(case_1)))\n",
    "print('sigmoid {}, softmax {}'.format(sigmoid(case_2), softmax(case_2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습과정 설정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', # 옵티마이저 : Adam\n",
    "              loss = 'categorical_crossentropy', # 손실 함수 : categorical_crossentropy\n",
    "              metrics=['acc']) # 모니터링 할 평가지표 : acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0928 15:31:14.036037 20628 deprecation.py:323] From C:\\Users\\jcy12\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/10\n",
      "42000/42000 [==============================] - 2s 42us/sample - loss: 0.4959 - acc: 0.8622 - val_loss: 0.2366 - val_acc: 0.9337\n",
      "Epoch 2/10\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 0.2041 - acc: 0.9399 - val_loss: 0.1816 - val_acc: 0.9482\n",
      "Epoch 3/10\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 0.1543 - acc: 0.9554 - val_loss: 0.1613 - val_acc: 0.9547\n",
      "Epoch 4/10\n",
      "42000/42000 [==============================] - 1s 31us/sample - loss: 0.1261 - acc: 0.9641 - val_loss: 0.1476 - val_acc: 0.9562\n",
      "Epoch 5/10\n",
      "42000/42000 [==============================] - 1s 31us/sample - loss: 0.1063 - acc: 0.9696 - val_loss: 0.1336 - val_acc: 0.9621\n",
      "Epoch 6/10\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.0904 - acc: 0.9734 - val_loss: 0.1288 - val_acc: 0.9623\n",
      "Epoch 7/10\n",
      "42000/42000 [==============================] - 1s 30us/sample - loss: 0.0781 - acc: 0.9767 - val_loss: 0.1159 - val_acc: 0.9651\n",
      "Epoch 8/10\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.0689 - acc: 0.9798 - val_loss: 0.1105 - val_acc: 0.9668\n",
      "Epoch 9/10\n",
      "42000/42000 [==============================] - 1s 29us/sample - loss: 0.0612 - acc: 0.9812 - val_loss: 0.1142 - val_acc: 0.9655\n",
      "Epoch 10/10\n",
      "42000/42000 [==============================] - 1s 31us/sample - loss: 0.0532 - acc: 0.9837 - val_loss: 0.1132 - val_acc: 0.9683\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, \n",
    "                    epochs = 10, \n",
    "                    batch_size = 128, \n",
    "                    validation_data = (x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## history를 통해 확인해볼 수 있는 값 출력하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 결과 그려보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "his_dict = history.history\n",
    "loss = his_dict['loss']\n",
    "val_loss = his_dict['val_loss'] # 검증 데이터가 있는 경우 ‘val_’ 수식어가 붙습니다.\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "fig = plt.figure(figsize = (10, 5))\n",
    "\n",
    "# 훈련 및 검증 손실 그리기\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax1.plot(epochs, loss, color = 'blue', label = 'train_loss')\n",
    "ax1.plot(epochs, val_loss, color = 'orange', label = 'val_loss')\n",
    "ax1.set_title('train and val loss')\n",
    "ax1.set_xlabel('epochs')\n",
    "ax1.set_ylabel('loss')\n",
    "ax1.legend()\n",
    "\n",
    "acc = his_dict['acc']\n",
    "val_acc = his_dict['val_acc']\n",
    "\n",
    "# 훈련 및 검증 정확도 그리기\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "ax2.plot(epochs, acc, color = 'blue', label = 'train_loss')\n",
    "ax2.plot(epochs, val_acc, color = 'orange', label = 'val_loss')\n",
    "ax2.set_title('train and val loss')\n",
    "ax2.set_xlabel('epochs')\n",
    "ax2.set_ylabel('loss')\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 58us/sample - loss: 0.1077 - acc: 0.9683\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.10774137125271373, 0.9683]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습된 모델을 통해 값 예측하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "results = model.predict(x_test)\n",
    "print(results.shape)\n",
    "np.set_printoptions(precision=7) # numpy 소수점 제한\n",
    "print('각 클래스에 속할 확률 : \\n{}'.format(results[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예측값 그려서 확인해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "arg_results = np.argmax(results, axis = -1) # 가장 큰 값의 인덱스를 가져옵니다.\n",
    "plt.imshow(x_test[0].reshape(28, 28))\n",
    "plt.title('Predicted value of the first image : ' + str(arg_results[0]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 평가 방법 1 - 혼동 행렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn.metrics 모듈은 여러가지 평가 지표에 관한 기능을 제공합니다.\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 혼동 행렬을 만듭니다.\n",
    "plt.figure(figsize = (7, 7))\n",
    "cm = confusion_matrix(np.argmax(y_test, axis = -1), np.argmax(results, axis = -1))\n",
    "sns.heatmap(cm, annot = True, fmt = 'd',cmap = 'Blues')\n",
    "plt.xlabel('predicted label')\n",
    "plt.ylabel('true label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 평가 방법 2 - 분류 보고서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n', classification_report(np.argmax(y_test, axis = -1), np.argmax(results, axis = -1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전체 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "tf.random.set_seed(777)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data(path='mnist.npz')\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = 0.3, random_state = 777)\n",
    "\n",
    "num_x_train = x_train.shape[0]\n",
    "num_x_val = x_val.shape[0]\n",
    "num_x_test = x_test.shape[0]\n",
    "\n",
    "x_train = (x_train.reshape((num_x_train, 28 * 28))) / 255\n",
    "x_val = (x_val.reshape((num_x_val, 28 * 28))) / 255\n",
    "x_test = (x_test.reshape((num_x_test, 28 * 28))) / 255\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_val = to_categorical(y_val)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation = 'relu', input_shape = (784, )))\n",
    "model.add(Dense(32, activation = 'relu'))\n",
    "model.add(Dense(10, activation = 'softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss = 'categorical_crossentropy', \n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model.fit(x_train, y_train, \n",
    "                    epochs = 30, \n",
    "                    batch_size = 128, \n",
    "                    validation_data = (x_val, y_val))\n",
    "\n",
    "# model.evaluate(x_test, y_test)\n",
    "results = model.predict(x_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras_study",
   "language": "python",
   "name": "keras_study"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
