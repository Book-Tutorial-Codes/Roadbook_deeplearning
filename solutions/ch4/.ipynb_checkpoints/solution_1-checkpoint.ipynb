{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1.\n",
    "MNIST 데이터셋은 복잡하지 않은 데이터셋이기 때문에 가벼운 환경에서도 다양한 실험을 해보기에 적합합니다.  \n",
    "필자는 계속해서 신경망이 스케일에 매우 민감하다고 언급해왔습니다.  \n",
    "MNIST 데이터셋에서의 스케일에 대한 전처리로 데이터를 255로 나누는 과정을 기억하나요?  \n",
    "이 과정을 거치지 않고 결과를 비교해보기 바랍니다. \n",
    "또한, 보스턴 주택 가격 예측 문제에서도 스케일 문제를 해결하기 위해 표준화를 진행해주었습니다.  \n",
    "표준화를 적용하지 않은 상태에서 신경망을 학습시켜보고 결과를 비교해보길 바랍니다.\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/30\n",
      "42000/42000 [==============================] - 1s 26us/sample - loss: 3.9865 - acc: 0.7166 - val_loss: 0.8133 - val_acc: 0.8292\n",
      "Epoch 2/30\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 0.6232 - acc: 0.8563 - val_loss: 0.5744 - val_acc: 0.8708\n",
      "Epoch 3/30\n",
      "42000/42000 [==============================] - 1s 19us/sample - loss: 0.4372 - acc: 0.8932 - val_loss: 0.4729 - val_acc: 0.8984\n",
      "Epoch 4/30\n",
      "42000/42000 [==============================] - 1s 19us/sample - loss: 0.3514 - acc: 0.9121 - val_loss: 0.3926 - val_acc: 0.9100\n",
      "Epoch 5/30\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 0.2983 - acc: 0.9235 - val_loss: 0.3687 - val_acc: 0.9152\n",
      "Epoch 6/30\n",
      "42000/42000 [==============================] - 1s 19us/sample - loss: 0.2638 - acc: 0.9304 - val_loss: 0.3585 - val_acc: 0.9170\n",
      "Epoch 7/30\n",
      "42000/42000 [==============================] - 1s 19us/sample - loss: 0.2460 - acc: 0.9341 - val_loss: 0.3228 - val_acc: 0.9237\n",
      "Epoch 8/30\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 0.2118 - acc: 0.9408 - val_loss: 0.3213 - val_acc: 0.9271\n",
      "Epoch 9/30\n",
      "42000/42000 [==============================] - 1s 19us/sample - loss: 0.2067 - acc: 0.9427 - val_loss: 0.3238 - val_acc: 0.9213\n",
      "Epoch 10/30\n",
      "42000/42000 [==============================] - 1s 19us/sample - loss: 0.1935 - acc: 0.9467 - val_loss: 0.3010 - val_acc: 0.9321\n",
      "Epoch 11/30\n",
      "42000/42000 [==============================] - 1s 19us/sample - loss: 0.1800 - acc: 0.9501 - val_loss: 0.2869 - val_acc: 0.9366\n",
      "Epoch 12/30\n",
      "42000/42000 [==============================] - 1s 19us/sample - loss: 0.1795 - acc: 0.9504 - val_loss: 0.2830 - val_acc: 0.9362\n",
      "Epoch 13/30\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 0.1721 - acc: 0.9526 - val_loss: 0.2862 - val_acc: 0.9381\n",
      "Epoch 14/30\n",
      "42000/42000 [==============================] - 1s 21us/sample - loss: 0.1642 - acc: 0.9544 - val_loss: 0.2779 - val_acc: 0.9379\n",
      "Epoch 15/30\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 0.1532 - acc: 0.9567 - val_loss: 0.2794 - val_acc: 0.9402\n",
      "Epoch 16/30\n",
      "42000/42000 [==============================] - 1s 19us/sample - loss: 0.1475 - acc: 0.9582 - val_loss: 0.2955 - val_acc: 0.9396\n",
      "Epoch 17/30\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 0.1372 - acc: 0.9602 - val_loss: 0.2899 - val_acc: 0.9397\n",
      "Epoch 18/30\n",
      "42000/42000 [==============================] - 1s 21us/sample - loss: 0.1348 - acc: 0.9624 - val_loss: 0.2593 - val_acc: 0.9441\n",
      "Epoch 19/30\n",
      "42000/42000 [==============================] - 1s 19us/sample - loss: 0.1331 - acc: 0.9628 - val_loss: 0.2797 - val_acc: 0.9389\n",
      "Epoch 20/30\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 0.1280 - acc: 0.9638 - val_loss: 0.2487 - val_acc: 0.9417\n",
      "Epoch 21/30\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 0.1248 - acc: 0.9641 - val_loss: 0.2885 - val_acc: 0.9416\n",
      "Epoch 22/30\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 0.1183 - acc: 0.9662 - val_loss: 0.2682 - val_acc: 0.9416\n",
      "Epoch 23/30\n",
      "42000/42000 [==============================] - 1s 21us/sample - loss: 0.1104 - acc: 0.9670 - val_loss: 0.2781 - val_acc: 0.9420\n",
      "Epoch 24/30\n",
      "42000/42000 [==============================] - 1s 19us/sample - loss: 0.1070 - acc: 0.9692 - val_loss: 0.2474 - val_acc: 0.9472\n",
      "Epoch 25/30\n",
      "42000/42000 [==============================] - 1s 21us/sample - loss: 0.0996 - acc: 0.9708 - val_loss: 0.2540 - val_acc: 0.9466\n",
      "Epoch 26/30\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 0.1037 - acc: 0.9697 - val_loss: 0.2756 - val_acc: 0.9461\n",
      "Epoch 27/30\n",
      "42000/42000 [==============================] - 1s 19us/sample - loss: 0.1009 - acc: 0.9710 - val_loss: 0.2484 - val_acc: 0.9480\n",
      "Epoch 28/30\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 0.0900 - acc: 0.9734 - val_loss: 0.2679 - val_acc: 0.9454\n",
      "Epoch 29/30\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 0.0887 - acc: 0.9740 - val_loss: 0.2543 - val_acc: 0.9468\n",
      "Epoch 30/30\n",
      "42000/42000 [==============================] - 1s 20us/sample - loss: 0.0836 - acc: 0.9755 - val_loss: 0.2651 - val_acc: 0.9504\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets.mnist import load_data\n",
    "\n",
    "# 텐서플로우 저장소에서 데이터를 다운받습니다.\n",
    "(x_train, y_train), (x_test, y_test) = load_data(path='mnist.npz')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 훈련/테스트 데이터를 0.7/0.3의 비율로 분리합니다.\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, \n",
    "                                                  test_size = 0.3, \n",
    "                                                  random_state = 777)\n",
    "\n",
    "num_x_train = x_train.shape[0]\n",
    "num_x_val = x_val.shape[0]\n",
    "num_x_test = x_test.shape[0]\n",
    "\n",
    "# 모델의 입력으로 사용하기 위한 전처리 과정입니다.\n",
    "# 전처리를 진행하지 않습니다.\n",
    "x_train = (x_train.reshape((num_x_train, 28 * 28)))\n",
    "x_val = (x_val.reshape((num_x_val, 28 * 28)))\n",
    "x_test = (x_test.reshape((num_x_test, 28 * 28)))\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# 각 데이터의 레이블을 범주형 형태로 변경합니다.\n",
    "y_train = to_categorical(y_train)\n",
    "y_val = to_categorical(y_val)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "# 입력 데이터의 형태를 꼭 명시해야 합니다.\n",
    "# 784차원의 데이터를 입력으로 받고, 64개의 출력을 가지는 첫 번째 Dense 층\n",
    "model.add(Dense(64, activation = 'relu', input_shape = (784, )))\n",
    "model.add(Dense(32, activation = 'relu')) # 32개의 출력을 가지는 Dense 층\n",
    "model.add(Dense(10, activation = 'softmax')) # 10개의 출력을 가지는 신경망\n",
    "\n",
    "model.compile(optimizer='adam', # 옵티마이저 : Adam\n",
    "              loss = 'categorical_crossentropy', # 손실 함수 : categorical_crossentropy\n",
    "              metrics=['acc']) # 모니터링 할 평가지표 : acc\n",
    "\n",
    "# 작은 차이라고 느껴질 수 있지만, 분명히 전처리를 수행한\n",
    "# 데이터셋을 학습하는 것이 성능이 더 좋습니다.\n",
    "history = model.fit(x_train, y_train, \n",
    "                    epochs = 30, \n",
    "                    batch_size = 128, \n",
    "                    validation_data = (x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boston Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
      "57344/57026 [==============================] - 0s 2us/step\n",
      "Train on 270 samples, validate on 134 samples\n",
      "Epoch 1/300\n",
      "270/270 [==============================] - 0s 1ms/sample - loss: 688.0070 - mae: 21.1392 - val_loss: 335.9643 - val_mae: 15.9830\n",
      "Epoch 2/300\n",
      "270/270 [==============================] - 0s 89us/sample - loss: 227.9749 - mae: 12.2799 - val_loss: 139.7444 - val_mae: 8.8430\n",
      "Epoch 3/300\n",
      "270/270 [==============================] - 0s 103us/sample - loss: 123.1663 - mae: 8.1964 - val_loss: 129.4582 - val_mae: 8.4281\n",
      "Epoch 4/300\n",
      "270/270 [==============================] - 0s 111us/sample - loss: 75.8382 - mae: 6.5995 - val_loss: 106.1535 - val_mae: 8.1574\n",
      "Epoch 5/300\n",
      "270/270 [==============================] - 0s 122us/sample - loss: 67.8701 - mae: 6.2011 - val_loss: 108.7167 - val_mae: 7.3002\n",
      "Epoch 6/300\n",
      "270/270 [==============================] - 0s 96us/sample - loss: 68.7437 - mae: 5.8609 - val_loss: 94.5150 - val_mae: 7.0046\n",
      "Epoch 7/300\n",
      "270/270 [==============================] - 0s 100us/sample - loss: 62.5460 - mae: 5.8954 - val_loss: 89.7141 - val_mae: 6.9492\n",
      "Epoch 8/300\n",
      "270/270 [==============================] - 0s 103us/sample - loss: 60.3356 - mae: 5.6117 - val_loss: 90.4164 - val_mae: 6.7559\n",
      "Epoch 9/300\n",
      "270/270 [==============================] - 0s 140us/sample - loss: 57.6933 - mae: 5.3663 - val_loss: 85.1223 - val_mae: 6.7045\n",
      "Epoch 10/300\n",
      "270/270 [==============================] - 0s 103us/sample - loss: 57.8327 - mae: 5.5647 - val_loss: 83.1846 - val_mae: 6.5162\n",
      "Epoch 11/300\n",
      "270/270 [==============================] - 0s 92us/sample - loss: 55.9002 - mae: 5.2533 - val_loss: 81.7742 - val_mae: 6.4729\n",
      "Epoch 12/300\n",
      "270/270 [==============================] - 0s 100us/sample - loss: 55.9224 - mae: 5.4943 - val_loss: 80.9690 - val_mae: 6.4264\n",
      "Epoch 13/300\n",
      "270/270 [==============================] - 0s 96us/sample - loss: 52.9717 - mae: 5.1635 - val_loss: 80.0217 - val_mae: 6.3764\n",
      "Epoch 14/300\n",
      "270/270 [==============================] - 0s 92us/sample - loss: 51.8159 - mae: 5.2023 - val_loss: 78.5695 - val_mae: 6.3424\n",
      "Epoch 15/300\n",
      "270/270 [==============================] - 0s 100us/sample - loss: 51.9529 - mae: 4.9914 - val_loss: 77.2906 - val_mae: 6.2928\n",
      "Epoch 16/300\n",
      "270/270 [==============================] - 0s 122us/sample - loss: 49.7262 - mae: 5.0104 - val_loss: 78.2989 - val_mae: 6.2005\n",
      "Epoch 17/300\n",
      "270/270 [==============================] - 0s 103us/sample - loss: 49.9599 - mae: 4.9985 - val_loss: 74.8626 - val_mae: 6.2960\n",
      "Epoch 18/300\n",
      "270/270 [==============================] - 0s 103us/sample - loss: 51.3576 - mae: 4.9131 - val_loss: 74.9463 - val_mae: 6.2579\n",
      "Epoch 19/300\n",
      "270/270 [==============================] - 0s 111us/sample - loss: 48.3347 - mae: 5.0362 - val_loss: 73.9158 - val_mae: 6.0589\n",
      "Epoch 20/300\n",
      "270/270 [==============================] - 0s 103us/sample - loss: 46.1631 - mae: 4.7565 - val_loss: 72.7535 - val_mae: 6.0063\n",
      "Epoch 21/300\n",
      "270/270 [==============================] - 0s 96us/sample - loss: 45.5819 - mae: 4.7597 - val_loss: 74.5165 - val_mae: 5.9724\n",
      "Epoch 22/300\n",
      "270/270 [==============================] - 0s 96us/sample - loss: 45.7104 - mae: 4.6813 - val_loss: 71.7088 - val_mae: 6.0021\n",
      "Epoch 23/300\n",
      "270/270 [==============================] - 0s 96us/sample - loss: 44.4164 - mae: 4.5781 - val_loss: 70.8906 - val_mae: 5.9932\n",
      "Epoch 24/300\n",
      "270/270 [==============================] - 0s 100us/sample - loss: 43.7009 - mae: 4.6145 - val_loss: 69.2255 - val_mae: 5.8965\n",
      "Epoch 25/300\n",
      "270/270 [==============================] - 0s 96us/sample - loss: 42.7784 - mae: 4.5308 - val_loss: 70.5526 - val_mae: 5.7978\n",
      "Epoch 26/300\n",
      "270/270 [==============================] - 0s 103us/sample - loss: 43.2059 - mae: 4.5210 - val_loss: 67.9746 - val_mae: 5.9294\n",
      "Epoch 27/300\n",
      "270/270 [==============================] - 0s 100us/sample - loss: 42.3179 - mae: 4.4519 - val_loss: 67.0115 - val_mae: 5.7894\n",
      "Epoch 28/300\n",
      "270/270 [==============================] - 0s 92us/sample - loss: 40.6369 - mae: 4.4561 - val_loss: 68.3729 - val_mae: 5.7545\n",
      "Epoch 29/300\n",
      "270/270 [==============================] - 0s 107us/sample - loss: 39.7034 - mae: 4.3235 - val_loss: 66.2323 - val_mae: 5.6487\n",
      "Epoch 30/300\n",
      "270/270 [==============================] - 0s 114us/sample - loss: 39.0463 - mae: 4.2007 - val_loss: 64.8125 - val_mae: 5.6369\n",
      "Epoch 31/300\n",
      "270/270 [==============================] - 0s 100us/sample - loss: 39.1029 - mae: 4.1891 - val_loss: 63.5815 - val_mae: 5.7097\n",
      "Epoch 32/300\n",
      "270/270 [==============================] - 0s 96us/sample - loss: 38.9284 - mae: 4.3292 - val_loss: 69.8713 - val_mae: 5.5411\n",
      "Epoch 33/300\n",
      "270/270 [==============================] - 0s 100us/sample - loss: 38.2125 - mae: 4.1576 - val_loss: 62.0414 - val_mae: 5.4909\n",
      "Epoch 34/300\n",
      "270/270 [==============================] - 0s 103us/sample - loss: 36.6860 - mae: 3.9875 - val_loss: 61.8352 - val_mae: 5.6203\n",
      "Epoch 35/300\n",
      "270/270 [==============================] - 0s 100us/sample - loss: 38.2209 - mae: 4.2773 - val_loss: 66.2601 - val_mae: 5.4156\n",
      "Epoch 36/300\n",
      "270/270 [==============================] - 0s 89us/sample - loss: 37.8772 - mae: 4.2971 - val_loss: 61.5443 - val_mae: 5.4143\n",
      "Epoch 37/300\n",
      "270/270 [==============================] - 0s 100us/sample - loss: 35.1894 - mae: 3.9162 - val_loss: 60.8567 - val_mae: 5.6078\n",
      "Epoch 38/300\n",
      "270/270 [==============================] - 0s 96us/sample - loss: 36.8489 - mae: 4.0307 - val_loss: 58.8964 - val_mae: 5.5558\n",
      "Epoch 39/300\n",
      "270/270 [==============================] - 0s 100us/sample - loss: 36.4710 - mae: 4.1963 - val_loss: 61.9168 - val_mae: 5.2810\n",
      "Epoch 40/300\n",
      "270/270 [==============================] - 0s 122us/sample - loss: 35.1270 - mae: 4.1007 - val_loss: 59.8211 - val_mae: 5.2693\n",
      "Epoch 41/300\n",
      "270/270 [==============================] - 0s 126us/sample - loss: 33.4013 - mae: 3.7643 - val_loss: 56.1537 - val_mae: 5.4152\n",
      "Epoch 42/300\n",
      "270/270 [==============================] - 0s 103us/sample - loss: 34.4912 - mae: 4.1199 - val_loss: 64.2330 - val_mae: 5.2355\n",
      "Epoch 43/300\n",
      "270/270 [==============================] - 0s 103us/sample - loss: 33.1388 - mae: 4.0035 - val_loss: 58.1778 - val_mae: 5.0982\n",
      "Epoch 44/300\n",
      "270/270 [==============================] - 0s 92us/sample - loss: 32.5123 - mae: 3.7702 - val_loss: 54.6222 - val_mae: 5.2230\n",
      "Epoch 45/300\n",
      "270/270 [==============================] - 0s 126us/sample - loss: 31.7459 - mae: 3.8400 - val_loss: 58.8225 - val_mae: 5.0353\n",
      "Epoch 46/300\n",
      "270/270 [==============================] - 0s 96us/sample - loss: 31.8051 - mae: 3.8267 - val_loss: 56.7190 - val_mae: 4.9910\n",
      "Epoch 47/300\n",
      "270/270 [==============================] - 0s 96us/sample - loss: 30.5965 - mae: 3.7012 - val_loss: 57.4547 - val_mae: 4.9727\n",
      "Epoch 48/300\n",
      "270/270 [==============================] - 0s 96us/sample - loss: 31.0664 - mae: 3.8017 - val_loss: 60.2689 - val_mae: 4.9848\n",
      "Epoch 49/300\n",
      "270/270 [==============================] - 0s 111us/sample - loss: 30.4465 - mae: 3.7067 - val_loss: 53.2494 - val_mae: 5.3782\n",
      "Epoch 50/300\n",
      "270/270 [==============================] - 0s 100us/sample - loss: 30.1824 - mae: 3.8561 - val_loss: 60.9121 - val_mae: 4.9927\n",
      "Epoch 51/300\n",
      "270/270 [==============================] - 0s 100us/sample - loss: 31.0372 - mae: 3.8165 - val_loss: 51.4472 - val_mae: 4.8620\n",
      "Epoch 52/300\n",
      "270/270 [==============================] - 0s 103us/sample - loss: 28.9697 - mae: 3.5603 - val_loss: 51.9455 - val_mae: 4.8795\n",
      "Epoch 53/300\n",
      "270/270 [==============================] - 0s 107us/sample - loss: 29.4215 - mae: 3.7786 - val_loss: 59.6164 - val_mae: 4.9432\n",
      "Epoch 54/300\n",
      "270/270 [==============================] - 0s 107us/sample - loss: 30.7107 - mae: 3.8958 - val_loss: 55.6859 - val_mae: 4.8184\n",
      "Epoch 55/300\n",
      "270/270 [==============================] - 0s 103us/sample - loss: 29.4195 - mae: 3.6010 - val_loss: 49.0816 - val_mae: 5.0214\n",
      "Epoch 56/300\n",
      "270/270 [==============================] - 0s 104us/sample - loss: 28.8618 - mae: 3.5985 - val_loss: 50.6471 - val_mae: 4.7345\n",
      "Epoch 57/300\n",
      "270/270 [==============================] - 0s 96us/sample - loss: 28.3723 - mae: 3.6029 - val_loss: 49.0423 - val_mae: 4.8714\n",
      "Epoch 58/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270/270 [==============================] - 0s 100us/sample - loss: 28.5155 - mae: 3.6546 - val_loss: 49.3893 - val_mae: 5.0191\n",
      "Epoch 59/300\n",
      "270/270 [==============================] - 0s 103us/sample - loss: 27.9792 - mae: 3.6008 - val_loss: 48.7806 - val_mae: 4.7479\n",
      "Epoch 60/300\n",
      "270/270 [==============================] - 0s 103us/sample - loss: 30.4615 - mae: 4.0141 - val_loss: 60.7912 - val_mae: 4.9934\n",
      "Epoch 61/300\n",
      "270/270 [==============================] - 0s 96us/sample - loss: 28.8997 - mae: 3.7380 - val_loss: 51.7598 - val_mae: 4.6480\n",
      "Epoch 62/300\n",
      "270/270 [==============================] - 0s 98us/sample - loss: 26.5041 - mae: 3.4326 - val_loss: 47.0502 - val_mae: 4.6141\n",
      "Epoch 63/300\n",
      "270/270 [==============================] - 0s 89us/sample - loss: 25.9825 - mae: 3.4973 - val_loss: 50.7716 - val_mae: 4.6055\n",
      "Epoch 64/300\n",
      "270/270 [==============================] - 0s 100us/sample - loss: 25.9179 - mae: 3.4360 - val_loss: 47.5285 - val_mae: 4.6995\n",
      "Epoch 65/300\n",
      "270/270 [==============================] - 0s 89us/sample - loss: 26.0318 - mae: 3.5542 - val_loss: 52.5045 - val_mae: 4.6698\n",
      "Epoch 66/300\n",
      "270/270 [==============================] - 0s 92us/sample - loss: 27.2553 - mae: 3.7139 - val_loss: 49.0857 - val_mae: 5.3764\n",
      "Epoch 67/300\n",
      "270/270 [==============================] - 0s 100us/sample - loss: 27.4923 - mae: 3.5651 - val_loss: 45.6184 - val_mae: 4.8126\n",
      "Epoch 68/300\n",
      "270/270 [==============================] - 0s 103us/sample - loss: 28.7813 - mae: 3.8464 - val_loss: 54.7788 - val_mae: 4.7392\n",
      "Epoch 69/300\n",
      "270/270 [==============================] - 0s 96us/sample - loss: 25.9587 - mae: 3.5031 - val_loss: 47.0175 - val_mae: 4.6852\n",
      "Epoch 70/300\n",
      "270/270 [==============================] - 0s 92us/sample - loss: 24.8158 - mae: 3.4407 - val_loss: 48.4734 - val_mae: 4.5944\n",
      "Epoch 71/300\n",
      "270/270 [==============================] - 0s 100us/sample - loss: 24.9296 - mae: 3.4688 - val_loss: 45.7912 - val_mae: 4.5742\n",
      "Epoch 72/300\n",
      "270/270 [==============================] - 0s 96us/sample - loss: 25.3547 - mae: 3.5171 - val_loss: 48.2543 - val_mae: 4.5860\n",
      "Epoch 73/300\n",
      "270/270 [==============================] - 0s 89us/sample - loss: 25.4373 - mae: 3.5110 - val_loss: 52.0681 - val_mae: 4.7093\n",
      "Epoch 74/300\n",
      "270/270 [==============================] - 0s 100us/sample - loss: 26.6234 - mae: 3.7649 - val_loss: 57.6988 - val_mae: 4.9358\n",
      "Epoch 75/300\n",
      "270/270 [==============================] - 0s 89us/sample - loss: 26.7377 - mae: 3.5399 - val_loss: 44.7344 - val_mae: 4.5617\n",
      "Epoch 76/300\n",
      "270/270 [==============================] - 0s 96us/sample - loss: 23.6271 - mae: 3.3796 - val_loss: 46.8891 - val_mae: 4.5270\n",
      "Epoch 77/300\n",
      "270/270 [==============================] - 0s 100us/sample - loss: 23.7308 - mae: 3.3888 - val_loss: 44.5796 - val_mae: 4.4004\n",
      "Epoch 78/300\n",
      "270/270 [==============================] - 0s 89us/sample - loss: 23.5884 - mae: 3.3274 - val_loss: 42.9620 - val_mae: 4.7895\n",
      "Epoch 79/300\n",
      "270/270 [==============================] - 0s 100us/sample - loss: 24.0700 - mae: 3.3977 - val_loss: 41.5487 - val_mae: 4.8671\n",
      "Epoch 80/300\n",
      "270/270 [==============================] - 0s 100us/sample - loss: 25.3225 - mae: 3.5740 - val_loss: 46.1816 - val_mae: 4.5516\n",
      "Epoch 81/300\n",
      "270/270 [==============================] - 0s 89us/sample - loss: 22.7171 - mae: 3.3502 - val_loss: 42.5314 - val_mae: 4.3229\n",
      "Epoch 82/300\n",
      "270/270 [==============================] - 0s 92us/sample - loss: 24.8631 - mae: 3.5648 - val_loss: 55.5599 - val_mae: 4.8943\n",
      "Epoch 83/300\n",
      "270/270 [==============================] - 0s 96us/sample - loss: 27.7551 - mae: 3.8092 - val_loss: 46.4176 - val_mae: 4.4333\n",
      "Epoch 84/300\n",
      "270/270 [==============================] - 0s 96us/sample - loss: 24.5091 - mae: 3.5167 - val_loss: 40.9804 - val_mae: 4.7062\n",
      "Epoch 85/300\n",
      "270/270 [==============================] - 0s 106us/sample - loss: 24.1401 - mae: 3.4124 - val_loss: 39.6731 - val_mae: 4.6848\n",
      "Epoch 86/300\n",
      "270/270 [==============================] - 0s 85us/sample - loss: 25.1295 - mae: 3.6765 - val_loss: 43.2267 - val_mae: 4.5233\n",
      "Epoch 87/300\n",
      "270/270 [==============================] - 0s 96us/sample - loss: 22.8181 - mae: 3.5003 - val_loss: 46.8633 - val_mae: 4.4342\n",
      "Epoch 88/300\n",
      "270/270 [==============================] - 0s 103us/sample - loss: 23.2290 - mae: 3.4408 - val_loss: 49.6509 - val_mae: 4.5631\n",
      "Epoch 89/300\n",
      "270/270 [==============================] - 0s 100us/sample - loss: 24.5837 - mae: 3.5379 - val_loss: 49.0724 - val_mae: 4.5484\n",
      "Epoch 90/300\n",
      "270/270 [==============================] - 0s 96us/sample - loss: 24.2439 - mae: 3.5146 - val_loss: 42.6074 - val_mae: 4.2899\n",
      "Epoch 91/300\n",
      "270/270 [==============================] - 0s 92us/sample - loss: 21.3284 - mae: 3.1934 - val_loss: 41.8650 - val_mae: 4.5063\n",
      "Epoch 92/300\n",
      "270/270 [==============================] - 0s 100us/sample - loss: 21.4660 - mae: 3.1728 - val_loss: 39.7122 - val_mae: 4.5130\n",
      "Epoch 93/300\n",
      "270/270 [==============================] - 0s 109us/sample - loss: 21.3982 - mae: 3.2365 - val_loss: 40.5629 - val_mae: 4.4817\n",
      "Epoch 94/300\n",
      "270/270 [==============================] - 0s 101us/sample - loss: 21.3560 - mae: 3.3241 - val_loss: 39.2462 - val_mae: 4.2143\n",
      "Epoch 95/300\n",
      "270/270 [==============================] - 0s 96us/sample - loss: 21.4922 - mae: 3.2115 - val_loss: 45.2047 - val_mae: 4.6403\n",
      "Epoch 96/300\n",
      "270/270 [==============================] - 0s 100us/sample - loss: 21.5389 - mae: 3.3313 - val_loss: 44.5633 - val_mae: 4.3574\n",
      "Epoch 97/300\n",
      "270/270 [==============================] - 0s 92us/sample - loss: 21.4115 - mae: 3.2741 - val_loss: 39.5976 - val_mae: 4.4384\n",
      "Epoch 98/300\n",
      "270/270 [==============================] - 0s 85us/sample - loss: 20.4415 - mae: 3.1655 - val_loss: 40.1625 - val_mae: 4.2909\n",
      "Epoch 99/300\n",
      "270/270 [==============================] - 0s 96us/sample - loss: 20.7983 - mae: 3.2017 - val_loss: 38.6639 - val_mae: 4.3534\n",
      "Epoch 100/300\n",
      "270/270 [==============================] - 0s 89us/sample - loss: 21.5635 - mae: 3.3091 - val_loss: 39.3230 - val_mae: 4.7771\n",
      "Epoch 101/300\n",
      "270/270 [==============================] - 0s 92us/sample - loss: 25.0128 - mae: 3.6584 - val_loss: 38.8201 - val_mae: 4.7527\n",
      "Epoch 102/300\n",
      "270/270 [==============================] - 0s 89us/sample - loss: 23.8840 - mae: 3.6734 - val_loss: 41.1057 - val_mae: 4.3492\n",
      "Epoch 103/300\n",
      "270/270 [==============================] - 0s 92us/sample - loss: 21.9664 - mae: 3.4297 - val_loss: 55.7994 - val_mae: 4.9538\n",
      "Epoch 104/300\n",
      "270/270 [==============================] - 0s 96us/sample - loss: 24.9440 - mae: 3.7960 - val_loss: 69.3482 - val_mae: 5.9470\n",
      "Epoch 105/300\n",
      "270/270 [==============================] - 0s 89us/sample - loss: 28.7432 - mae: 4.0694 - val_loss: 57.6310 - val_mae: 5.1010\n",
      "Epoch 106/300\n",
      "270/270 [==============================] - 0s 89us/sample - loss: 27.2694 - mae: 3.8212 - val_loss: 41.4058 - val_mae: 4.2302\n",
      "Epoch 107/300\n",
      "270/270 [==============================] - 0s 89us/sample - loss: 22.3484 - mae: 3.2887 - val_loss: 48.1733 - val_mae: 4.4633\n",
      "Epoch 108/300\n",
      "270/270 [==============================] - 0s 96us/sample - loss: 22.2430 - mae: 3.4315 - val_loss: 39.1917 - val_mae: 4.1829\n",
      "Epoch 109/300\n",
      "270/270 [==============================] - 0s 89us/sample - loss: 26.9040 - mae: 3.8209 - val_loss: 42.7497 - val_mae: 4.2894\n",
      "Epoch 110/300\n",
      "270/270 [==============================] - 0s 103us/sample - loss: 22.8005 - mae: 3.3698 - val_loss: 39.1258 - val_mae: 4.8317\n",
      "Epoch 111/300\n",
      "270/270 [==============================] - 0s 93us/sample - loss: 22.9134 - mae: 3.4302 - val_loss: 38.9596 - val_mae: 4.7536\n",
      "Epoch 112/300\n",
      "270/270 [==============================] - 0s 109us/sample - loss: 23.7280 - mae: 3.6239 - val_loss: 37.7786 - val_mae: 4.3526\n",
      "Epoch 113/300\n",
      "270/270 [==============================] - 0s 118us/sample - loss: 20.9281 - mae: 3.2325 - val_loss: 38.6474 - val_mae: 4.6594\n",
      "Epoch 114/300\n",
      "270/270 [==============================] - 0s 107us/sample - loss: 22.9986 - mae: 3.5362 - val_loss: 38.3660 - val_mae: 4.2168\n",
      "Epoch 115/300\n",
      "270/270 [==============================] - 0s 92us/sample - loss: 20.8476 - mae: 3.2177 - val_loss: 36.9593 - val_mae: 4.3316\n",
      "Epoch 116/300\n",
      "270/270 [==============================] - 0s 92us/sample - loss: 20.8202 - mae: 3.3926 - val_loss: 47.0023 - val_mae: 4.4247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117/300\n",
      "270/270 [==============================] - 0s 89us/sample - loss: 19.4183 - mae: 3.1342 - val_loss: 41.5764 - val_mae: 4.2593\n",
      "Epoch 118/300\n",
      "270/270 [==============================] - 0s 89us/sample - loss: 19.0788 - mae: 3.0543 - val_loss: 37.1858 - val_mae: 4.6650\n",
      "Epoch 119/300\n",
      "270/270 [==============================] - 0s 107us/sample - loss: 22.2414 - mae: 3.4976 - val_loss: 35.8677 - val_mae: 4.3257\n",
      "Epoch 120/300\n",
      "270/270 [==============================] - 0s 99us/sample - loss: 23.3267 - mae: 3.5230 - val_loss: 42.7054 - val_mae: 5.1794\n",
      "Epoch 121/300\n",
      "270/270 [==============================] - 0s 89us/sample - loss: 27.6622 - mae: 4.0076 - val_loss: 36.9245 - val_mae: 4.2574\n",
      "Epoch 122/300\n",
      "270/270 [==============================] - 0s 89us/sample - loss: 20.7996 - mae: 3.4129 - val_loss: 47.4415 - val_mae: 4.4908\n",
      "Epoch 123/300\n",
      "270/270 [==============================] - 0s 89us/sample - loss: 20.3910 - mae: 3.2924 - val_loss: 43.8461 - val_mae: 4.2311\n",
      "Epoch 124/300\n",
      "270/270 [==============================] - 0s 107us/sample - loss: 19.3873 - mae: 3.1697 - val_loss: 50.7624 - val_mae: 4.6604\n",
      "Epoch 125/300\n",
      "270/270 [==============================] - 0s 100us/sample - loss: 22.0766 - mae: 3.4744 - val_loss: 39.5468 - val_mae: 4.0964\n",
      "Epoch 126/300\n",
      "270/270 [==============================] - 0s 89us/sample - loss: 20.0580 - mae: 3.2652 - val_loss: 41.0038 - val_mae: 4.0969\n",
      "Epoch 127/300\n",
      "270/270 [==============================] - 0s 92us/sample - loss: 20.2152 - mae: 3.2698 - val_loss: 50.4848 - val_mae: 4.7464\n",
      "Epoch 128/300\n",
      "270/270 [==============================] - 0s 85us/sample - loss: 18.9599 - mae: 3.1428 - val_loss: 41.0537 - val_mae: 4.1271\n",
      "Epoch 129/300\n",
      "270/270 [==============================] - 0s 92us/sample - loss: 18.3564 - mae: 3.1045 - val_loss: 36.8718 - val_mae: 4.3155\n",
      "Epoch 130/300\n",
      "270/270 [==============================] - 0s 100us/sample - loss: 18.3120 - mae: 3.0649 - val_loss: 41.4070 - val_mae: 4.1983\n",
      "Epoch 131/300\n",
      "270/270 [==============================] - 0s 92us/sample - loss: 18.9188 - mae: 3.0904 - val_loss: 37.1688 - val_mae: 4.2517\n",
      "Epoch 132/300\n",
      "270/270 [==============================] - 0s 89us/sample - loss: 19.5497 - mae: 3.1136 - val_loss: 37.4890 - val_mae: 4.3204\n",
      "Epoch 133/300\n",
      "270/270 [==============================] - 0s 89us/sample - loss: 19.8238 - mae: 3.2235 - val_loss: 36.5433 - val_mae: 4.1762\n",
      "Epoch 134/300\n",
      "270/270 [==============================] - 0s 92us/sample - loss: 20.3247 - mae: 3.2906 - val_loss: 35.7231 - val_mae: 4.6284\n",
      "Epoch 135/300\n",
      "270/270 [==============================] - 0s 96us/sample - loss: 20.1018 - mae: 3.1685 - val_loss: 37.7068 - val_mae: 4.5561\n",
      "Epoch 136/300\n",
      "270/270 [==============================] - 0s 92us/sample - loss: 19.1915 - mae: 3.0835 - val_loss: 34.9962 - val_mae: 4.4545\n",
      "Epoch 137/300\n",
      "270/270 [==============================] - 0s 85us/sample - loss: 21.7762 - mae: 3.4811 - val_loss: 34.7202 - val_mae: 4.3406\n",
      "Epoch 138/300\n",
      "270/270 [==============================] - 0s 85us/sample - loss: 19.1298 - mae: 3.1588 - val_loss: 37.7638 - val_mae: 4.4075\n",
      "Epoch 139/300\n",
      "270/270 [==============================] - 0s 89us/sample - loss: 18.8630 - mae: 3.1707 - val_loss: 36.8169 - val_mae: 3.9577\n",
      "Epoch 140/300\n",
      "270/270 [==============================] - 0s 100us/sample - loss: 18.7856 - mae: 3.1026 - val_loss: 35.5780 - val_mae: 4.1230\n",
      "Epoch 141/300\n",
      "270/270 [==============================] - 0s 92us/sample - loss: 19.5001 - mae: 3.2154 - val_loss: 37.0951 - val_mae: 4.4371\n",
      "Epoch 142/300\n",
      "270/270 [==============================] - 0s 96us/sample - loss: 17.9477 - mae: 2.9734 - val_loss: 35.5546 - val_mae: 4.1763\n",
      "Epoch 143/300\n",
      "270/270 [==============================] - 0s 96us/sample - loss: 17.9691 - mae: 3.1088 - val_loss: 35.8780 - val_mae: 4.1839\n",
      "Epoch 144/300\n",
      "270/270 [==============================] - 0s 107us/sample - loss: 18.3150 - mae: 3.0815 - val_loss: 37.2804 - val_mae: 4.9064\n",
      "Epoch 145/300\n",
      "270/270 [==============================] - 0s 100us/sample - loss: 20.1583 - mae: 3.2726 - val_loss: 36.2727 - val_mae: 4.6177\n",
      "Epoch 146/300\n",
      "270/270 [==============================] - 0s 100us/sample - loss: 20.9156 - mae: 3.3560 - val_loss: 34.8313 - val_mae: 4.5979\n",
      "Epoch 147/300\n",
      "270/270 [==============================] - 0s 103us/sample - loss: 19.5755 - mae: 3.2745 - val_loss: 37.2583 - val_mae: 4.7344\n",
      "Epoch 148/300\n",
      "270/270 [==============================] - 0s 92us/sample - loss: 17.8160 - mae: 3.1861 - val_loss: 35.6082 - val_mae: 3.9897\n",
      "Epoch 149/300\n",
      "270/270 [==============================] - 0s 100us/sample - loss: 17.1043 - mae: 2.9130 - val_loss: 39.1644 - val_mae: 4.2472\n",
      "Epoch 150/300\n",
      "270/270 [==============================] - 0s 103us/sample - loss: 16.7799 - mae: 2.8802 - val_loss: 34.8700 - val_mae: 3.9422\n",
      "Epoch 151/300\n",
      "270/270 [==============================] - 0s 92us/sample - loss: 18.7958 - mae: 3.1688 - val_loss: 38.5960 - val_mae: 4.3094\n",
      "Epoch 152/300\n",
      "270/270 [==============================] - 0s 100us/sample - loss: 18.1096 - mae: 3.1624 - val_loss: 36.1152 - val_mae: 3.9410\n",
      "Epoch 153/300\n",
      "270/270 [==============================] - 0s 111us/sample - loss: 18.2190 - mae: 3.0758 - val_loss: 44.5120 - val_mae: 4.6859\n",
      "Epoch 154/300\n",
      "270/270 [==============================] - 0s 92us/sample - loss: 18.6598 - mae: 3.1578 - val_loss: 34.8256 - val_mae: 4.1352\n",
      "Epoch 155/300\n",
      "270/270 [==============================] - 0s 92us/sample - loss: 17.5528 - mae: 2.9715 - val_loss: 38.6670 - val_mae: 4.2737\n",
      "Epoch 156/300\n",
      "270/270 [==============================] - 0s 103us/sample - loss: 17.8366 - mae: 3.0554 - val_loss: 35.9797 - val_mae: 4.1297\n",
      "Epoch 157/300\n",
      "270/270 [==============================] - 0s 92us/sample - loss: 16.7527 - mae: 2.9831 - val_loss: 37.7790 - val_mae: 3.9716\n",
      "Epoch 158/300\n",
      "270/270 [==============================] - 0s 96us/sample - loss: 17.9283 - mae: 3.1498 - val_loss: 47.2241 - val_mae: 4.5524\n",
      "Epoch 159/300\n",
      "270/270 [==============================] - 0s 89us/sample - loss: 18.0185 - mae: 3.1884 - val_loss: 49.5269 - val_mae: 4.5886\n",
      "Epoch 160/300\n",
      "270/270 [==============================] - 0s 96us/sample - loss: 19.7086 - mae: 3.2349 - val_loss: 36.8752 - val_mae: 3.9973\n",
      "Epoch 161/300\n",
      "270/270 [==============================] - 0s 96us/sample - loss: 18.0381 - mae: 3.1032 - val_loss: 37.6971 - val_mae: 4.0626\n",
      "Epoch 162/300\n",
      "270/270 [==============================] - 0s 92us/sample - loss: 18.7543 - mae: 3.1073 - val_loss: 33.9140 - val_mae: 4.2804\n",
      "Epoch 163/300\n",
      "270/270 [==============================] - 0s 100us/sample - loss: 17.3165 - mae: 3.0990 - val_loss: 41.8999 - val_mae: 4.2718\n",
      "Epoch 164/300\n",
      "270/270 [==============================] - 0s 96us/sample - loss: 20.5738 - mae: 3.4280 - val_loss: 42.3883 - val_mae: 4.2123\n",
      "Epoch 165/300\n",
      "270/270 [==============================] - 0s 92us/sample - loss: 20.8418 - mae: 3.4159 - val_loss: 42.7430 - val_mae: 4.4854\n",
      "Epoch 166/300\n",
      "270/270 [==============================] - 0s 100us/sample - loss: 17.1217 - mae: 3.0090 - val_loss: 36.3768 - val_mae: 3.9629\n",
      "Epoch 167/300\n",
      "270/270 [==============================] - 0s 100us/sample - loss: 16.8269 - mae: 2.9267 - val_loss: 33.3487 - val_mae: 4.0372\n",
      "Epoch 168/300\n",
      "270/270 [==============================] - 0s 92us/sample - loss: 17.4105 - mae: 3.0591 - val_loss: 37.7080 - val_mae: 4.7672\n",
      "Epoch 169/300\n",
      "270/270 [==============================] - 0s 100us/sample - loss: 20.6141 - mae: 3.4298 - val_loss: 33.8745 - val_mae: 4.3434\n",
      "Epoch 170/300\n",
      "270/270 [==============================] - 0s 96us/sample - loss: 18.2207 - mae: 3.1857 - val_loss: 40.4875 - val_mae: 4.1549\n",
      "Epoch 171/300\n",
      "270/270 [==============================] - 0s 89us/sample - loss: 16.8770 - mae: 2.8688 - val_loss: 35.3775 - val_mae: 4.1377\n",
      "Epoch 172/300\n",
      "270/270 [==============================] - 0s 100us/sample - loss: 16.6187 - mae: 2.9396 - val_loss: 34.0659 - val_mae: 4.6024\n",
      "Epoch 173/300\n",
      "270/270 [==============================] - 0s 85us/sample - loss: 20.9945 - mae: 3.4246 - val_loss: 33.3120 - val_mae: 4.3423\n",
      "Epoch 174/300\n",
      "270/270 [==============================] - 0s 100us/sample - loss: 20.2840 - mae: 3.4712 - val_loss: 34.7983 - val_mae: 3.9270\n",
      "Epoch 175/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270/270 [==============================] - 0s 100us/sample - loss: 18.2675 - mae: 3.2177 - val_loss: 40.3427 - val_mae: 4.2843\n",
      "Epoch 176/300\n",
      "270/270 [==============================] - 0s 89us/sample - loss: 16.6290 - mae: 2.9719 - val_loss: 34.7906 - val_mae: 3.9261\n",
      "Epoch 177/300\n",
      "270/270 [==============================] - 0s 106us/sample - loss: 19.8786 - mae: 3.3031 - val_loss: 32.6666 - val_mae: 4.1010\n",
      "Epoch 178/300\n",
      "270/270 [==============================] - 0s 96us/sample - loss: 17.8102 - mae: 3.1547 - val_loss: 34.0909 - val_mae: 4.1028\n",
      "Epoch 179/300\n",
      "270/270 [==============================] - 0s 96us/sample - loss: 16.8629 - mae: 2.9742 - val_loss: 36.9495 - val_mae: 4.1225\n",
      "Epoch 180/300\n",
      "270/270 [==============================] - 0s 100us/sample - loss: 15.6907 - mae: 2.8674 - val_loss: 33.0847 - val_mae: 4.3515\n",
      "Epoch 181/300\n",
      "270/270 [==============================] - 0s 96us/sample - loss: 17.2397 - mae: 3.0237 - val_loss: 34.0372 - val_mae: 4.3639\n",
      "Epoch 182/300\n",
      "270/270 [==============================] - 0s 92us/sample - loss: 16.7282 - mae: 3.0159 - val_loss: 33.7834 - val_mae: 3.9023\n",
      "Epoch 183/300\n",
      "270/270 [==============================] - 0s 97us/sample - loss: 15.8565 - mae: 2.9031 - val_loss: 35.7899 - val_mae: 4.1649\n",
      "Epoch 184/300\n",
      "270/270 [==============================] - 0s 85us/sample - loss: 15.2912 - mae: 2.8094 - val_loss: 34.9255 - val_mae: 3.9217\n",
      "Epoch 185/300\n",
      "270/270 [==============================] - 0s 96us/sample - loss: 16.1401 - mae: 2.8535 - val_loss: 34.5514 - val_mae: 3.9374\n",
      "Epoch 186/300\n",
      "270/270 [==============================] - 0s 100us/sample - loss: 16.5996 - mae: 2.9968 - val_loss: 36.6684 - val_mae: 4.1247\n",
      "Epoch 187/300\n",
      "270/270 [==============================] - 0s 113us/sample - loss: 16.1523 - mae: 2.9889 - val_loss: 34.1434 - val_mae: 3.8856\n",
      "Epoch 188/300\n",
      "270/270 [==============================] - 0s 115us/sample - loss: 16.9380 - mae: 3.0059 - val_loss: 41.4738 - val_mae: 4.4413\n",
      "Epoch 189/300\n",
      "270/270 [==============================] - 0s 96us/sample - loss: 15.5719 - mae: 2.8163 - val_loss: 33.8692 - val_mae: 4.0293\n",
      "Epoch 190/300\n",
      "270/270 [==============================] - 0s 96us/sample - loss: 15.5491 - mae: 2.9018 - val_loss: 36.8569 - val_mae: 4.0483\n",
      "Epoch 191/300\n",
      "270/270 [==============================] - 0s 92us/sample - loss: 15.2256 - mae: 2.8437 - val_loss: 40.2353 - val_mae: 4.1599\n",
      "Epoch 192/300\n",
      "270/270 [==============================] - 0s 96us/sample - loss: 16.3876 - mae: 2.8252 - val_loss: 36.8653 - val_mae: 3.9833\n",
      "Epoch 193/300\n",
      "270/270 [==============================] - 0s 96us/sample - loss: 15.0388 - mae: 2.8528 - val_loss: 39.6325 - val_mae: 4.1673\n",
      "Epoch 194/300\n",
      "270/270 [==============================] - 0s 111us/sample - loss: 15.8049 - mae: 2.9437 - val_loss: 41.2999 - val_mae: 4.2430\n",
      "Epoch 195/300\n",
      "270/270 [==============================] - 0s 100us/sample - loss: 15.7182 - mae: 2.9066 - val_loss: 39.0517 - val_mae: 4.1112\n",
      "Epoch 196/300\n",
      "270/270 [==============================] - 0s 100us/sample - loss: 16.3223 - mae: 2.9292 - val_loss: 38.1097 - val_mae: 4.2756\n",
      "Epoch 197/300\n",
      "270/270 [==============================] - 0s 92us/sample - loss: 16.5416 - mae: 2.9789 - val_loss: 31.9910 - val_mae: 4.4498\n",
      "Epoch 198/300\n",
      "270/270 [==============================] - 0s 89us/sample - loss: 17.5303 - mae: 3.1134 - val_loss: 32.3979 - val_mae: 4.3675\n",
      "Epoch 199/300\n",
      "270/270 [==============================] - 0s 94us/sample - loss: 18.5986 - mae: 3.2576 - val_loss: 34.0682 - val_mae: 4.5287\n",
      "Epoch 200/300\n",
      "270/270 [==============================] - 0s 85us/sample - loss: 16.2399 - mae: 2.9444 - val_loss: 32.3782 - val_mae: 3.9286\n",
      "Epoch 201/300\n",
      "270/270 [==============================] - 0s 96us/sample - loss: 15.5839 - mae: 2.8493 - val_loss: 40.1144 - val_mae: 4.5098\n",
      "Epoch 202/300\n",
      "270/270 [==============================] - 0s 89us/sample - loss: 16.2011 - mae: 2.8931 - val_loss: 33.6254 - val_mae: 3.8858\n",
      "Epoch 203/300\n",
      "270/270 [==============================] - 0s 89us/sample - loss: 16.2212 - mae: 2.8978 - val_loss: 34.9531 - val_mae: 3.9601\n",
      "Epoch 204/300\n",
      "270/270 [==============================] - 0s 85us/sample - loss: 17.0176 - mae: 3.0269 - val_loss: 39.2204 - val_mae: 4.0718\n",
      "Epoch 205/300\n",
      "270/270 [==============================] - 0s 103us/sample - loss: 21.0964 - mae: 3.4484 - val_loss: 48.3232 - val_mae: 4.8101\n",
      "Epoch 206/300\n",
      "270/270 [==============================] - 0s 100us/sample - loss: 19.2162 - mae: 3.1994 - val_loss: 38.3943 - val_mae: 4.0514\n",
      "Epoch 207/300\n",
      "270/270 [==============================] - 0s 92us/sample - loss: 16.0805 - mae: 2.9550 - val_loss: 44.4043 - val_mae: 4.5016\n",
      "Epoch 208/300\n",
      "270/270 [==============================] - 0s 100us/sample - loss: 18.1266 - mae: 3.1673 - val_loss: 41.1955 - val_mae: 4.1647\n",
      "Epoch 209/300\n",
      "270/270 [==============================] - 0s 103us/sample - loss: 15.4625 - mae: 2.9344 - val_loss: 44.2781 - val_mae: 4.4095\n",
      "Epoch 210/300\n",
      "270/270 [==============================] - 0s 89us/sample - loss: 17.9942 - mae: 3.1512 - val_loss: 46.3326 - val_mae: 4.6548\n",
      "Epoch 211/300\n",
      "270/270 [==============================] - 0s 100us/sample - loss: 16.5013 - mae: 3.0797 - val_loss: 42.5294 - val_mae: 4.3190\n",
      "Epoch 212/300\n",
      "270/270 [==============================] - 0s 96us/sample - loss: 19.1166 - mae: 3.3350 - val_loss: 52.9372 - val_mae: 5.0314\n",
      "Epoch 213/300\n",
      "270/270 [==============================] - 0s 88us/sample - loss: 17.9442 - mae: 3.2689 - val_loss: 41.7783 - val_mae: 4.3828\n",
      "Epoch 214/300\n",
      "270/270 [==============================] - 0s 104us/sample - loss: 15.5754 - mae: 2.8271 - val_loss: 33.1267 - val_mae: 3.9307\n",
      "Epoch 215/300\n",
      "270/270 [==============================] - 0s 97us/sample - loss: 14.4234 - mae: 2.7511 - val_loss: 32.9647 - val_mae: 4.1000\n",
      "Epoch 216/300\n",
      "270/270 [==============================] - 0s 103us/sample - loss: 15.1688 - mae: 2.8112 - val_loss: 34.2831 - val_mae: 4.0065\n",
      "Epoch 217/300\n",
      "270/270 [==============================] - 0s 103us/sample - loss: 14.8265 - mae: 2.7320 - val_loss: 33.2422 - val_mae: 3.8395\n",
      "Epoch 218/300\n",
      "270/270 [==============================] - 0s 92us/sample - loss: 15.2181 - mae: 2.8927 - val_loss: 47.5012 - val_mae: 4.7135\n",
      "Epoch 219/300\n",
      "270/270 [==============================] - 0s 100us/sample - loss: 15.5001 - mae: 2.8969 - val_loss: 35.0801 - val_mae: 3.9353\n",
      "Epoch 220/300\n",
      "270/270 [==============================] - 0s 92us/sample - loss: 14.7243 - mae: 2.7848 - val_loss: 38.7797 - val_mae: 4.1955\n",
      "Epoch 221/300\n",
      "270/270 [==============================] - 0s 103us/sample - loss: 15.9404 - mae: 2.8335 - val_loss: 36.1013 - val_mae: 3.9659\n",
      "Epoch 222/300\n",
      "270/270 [==============================] - 0s 92us/sample - loss: 14.7254 - mae: 2.7835 - val_loss: 33.5567 - val_mae: 3.8803\n",
      "Epoch 223/300\n",
      "270/270 [==============================] - 0s 100us/sample - loss: 15.5173 - mae: 2.8322 - val_loss: 33.3056 - val_mae: 3.9320\n",
      "Epoch 224/300\n",
      "270/270 [==============================] - 0s 94us/sample - loss: 14.7229 - mae: 2.8332 - val_loss: 40.1418 - val_mae: 4.1971\n",
      "Epoch 225/300\n",
      "270/270 [==============================] - 0s 96us/sample - loss: 16.1520 - mae: 2.9605 - val_loss: 44.0764 - val_mae: 4.5005\n",
      "Epoch 226/300\n",
      "270/270 [==============================] - 0s 96us/sample - loss: 17.5993 - mae: 3.2085 - val_loss: 41.1329 - val_mae: 4.2565\n",
      "Epoch 227/300\n",
      "270/270 [==============================] - 0s 100us/sample - loss: 18.7585 - mae: 3.1685 - val_loss: 41.2968 - val_mae: 4.2200\n",
      "Epoch 228/300\n",
      "270/270 [==============================] - 0s 95us/sample - loss: 15.6676 - mae: 2.9597 - val_loss: 41.9727 - val_mae: 4.4816\n",
      "Epoch 229/300\n",
      "270/270 [==============================] - 0s 100us/sample - loss: 15.6320 - mae: 2.8847 - val_loss: 33.8815 - val_mae: 3.9091\n",
      "Epoch 230/300\n",
      "270/270 [==============================] - 0s 89us/sample - loss: 14.8426 - mae: 2.7707 - val_loss: 30.1796 - val_mae: 4.0577\n",
      "Epoch 231/300\n",
      "270/270 [==============================] - 0s 100us/sample - loss: 15.9882 - mae: 2.9045 - val_loss: 33.5029 - val_mae: 4.0274\n",
      "Epoch 232/300\n",
      "270/270 [==============================] - 0s 111us/sample - loss: 16.7563 - mae: 2.9908 - val_loss: 36.3012 - val_mae: 4.9132\n",
      "Epoch 233/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270/270 [==============================] - 0s 92us/sample - loss: 21.6484 - mae: 3.6304 - val_loss: 31.1486 - val_mae: 4.4671\n",
      "Epoch 234/300\n",
      "270/270 [==============================] - 0s 103us/sample - loss: 17.1973 - mae: 3.0213 - val_loss: 40.4071 - val_mae: 5.1044\n",
      "Epoch 235/300\n",
      "270/270 [==============================] - 0s 92us/sample - loss: 17.5547 - mae: 3.2328 - val_loss: 31.7157 - val_mae: 4.0104\n",
      "Epoch 236/300\n",
      "270/270 [==============================] - 0s 89us/sample - loss: 15.2743 - mae: 2.8466 - val_loss: 33.1258 - val_mae: 4.0466\n",
      "Epoch 237/300\n",
      "270/270 [==============================] - 0s 92us/sample - loss: 14.9614 - mae: 2.8369 - val_loss: 31.5291 - val_mae: 3.9118\n",
      "Epoch 238/300\n",
      "270/270 [==============================] - 0s 96us/sample - loss: 14.2358 - mae: 2.6876 - val_loss: 33.6650 - val_mae: 4.0002\n",
      "Epoch 239/300\n",
      "270/270 [==============================] - 0s 100us/sample - loss: 14.6972 - mae: 2.8700 - val_loss: 35.4277 - val_mae: 3.8871\n",
      "Epoch 240/300\n",
      "270/270 [==============================] - 0s 100us/sample - loss: 15.0190 - mae: 2.7842 - val_loss: 38.4707 - val_mae: 4.3397\n",
      "Epoch 241/300\n",
      "270/270 [==============================] - 0s 96us/sample - loss: 15.0750 - mae: 2.7837 - val_loss: 38.6307 - val_mae: 4.1165\n",
      "Epoch 242/300\n",
      "270/270 [==============================] - 0s 107us/sample - loss: 14.0187 - mae: 2.7218 - val_loss: 32.0464 - val_mae: 4.1355\n",
      "Epoch 243/300\n",
      "270/270 [==============================] - 0s 92us/sample - loss: 16.9973 - mae: 3.0988 - val_loss: 29.5117 - val_mae: 3.9286\n",
      "Epoch 244/300\n",
      "270/270 [==============================] - 0s 96us/sample - loss: 20.1365 - mae: 3.3156 - val_loss: 43.3284 - val_mae: 4.5405\n",
      "Epoch 245/300\n",
      "270/270 [==============================] - 0s 92us/sample - loss: 15.8754 - mae: 2.9155 - val_loss: 32.2810 - val_mae: 3.8068\n",
      "Epoch 246/300\n",
      "270/270 [==============================] - 0s 104us/sample - loss: 14.3430 - mae: 2.7652 - val_loss: 37.0684 - val_mae: 3.9989\n",
      "Epoch 247/300\n",
      "270/270 [==============================] - 0s 100us/sample - loss: 15.4692 - mae: 2.8670 - val_loss: 40.9650 - val_mae: 4.4049\n",
      "Epoch 248/300\n",
      "270/270 [==============================] - 0s 100us/sample - loss: 15.6330 - mae: 2.9770 - val_loss: 37.5268 - val_mae: 4.0328\n",
      "Epoch 249/300\n",
      "270/270 [==============================] - 0s 89us/sample - loss: 16.6013 - mae: 2.9271 - val_loss: 39.2123 - val_mae: 4.2496\n",
      "Epoch 250/300\n",
      "270/270 [==============================] - 0s 107us/sample - loss: 13.8494 - mae: 2.7431 - val_loss: 38.7265 - val_mae: 4.0917\n",
      "Epoch 251/300\n",
      "270/270 [==============================] - 0s 115us/sample - loss: 17.0834 - mae: 3.0402 - val_loss: 32.4578 - val_mae: 4.1129\n",
      "Epoch 252/300\n",
      "270/270 [==============================] - 0s 100us/sample - loss: 17.5304 - mae: 3.2067 - val_loss: 30.1840 - val_mae: 3.8185\n",
      "Epoch 253/300\n",
      "270/270 [==============================] - 0s 92us/sample - loss: 16.7516 - mae: 2.9973 - val_loss: 32.0715 - val_mae: 3.9509\n",
      "Epoch 254/300\n",
      "270/270 [==============================] - 0s 92us/sample - loss: 15.4807 - mae: 2.8663 - val_loss: 34.4213 - val_mae: 4.8866\n",
      "Epoch 255/300\n",
      "270/270 [==============================] - 0s 96us/sample - loss: 19.4933 - mae: 3.4445 - val_loss: 29.6819 - val_mae: 4.0652\n",
      "Epoch 256/300\n",
      "270/270 [==============================] - 0s 96us/sample - loss: 14.4767 - mae: 2.7395 - val_loss: 33.2950 - val_mae: 4.0755\n",
      "Epoch 257/300\n",
      "270/270 [==============================] - 0s 103us/sample - loss: 15.8971 - mae: 2.9385 - val_loss: 37.2253 - val_mae: 3.9832\n",
      "Epoch 258/300\n",
      "270/270 [==============================] - 0s 98us/sample - loss: 14.4909 - mae: 2.7889 - val_loss: 33.9109 - val_mae: 3.9631\n",
      "Epoch 259/300\n",
      "270/270 [==============================] - 0s 92us/sample - loss: 14.2802 - mae: 2.8144 - val_loss: 37.0717 - val_mae: 3.9761\n",
      "Epoch 260/300\n",
      "270/270 [==============================] - 0s 96us/sample - loss: 14.9136 - mae: 2.8724 - val_loss: 41.6520 - val_mae: 4.5181\n",
      "Epoch 261/300\n",
      "270/270 [==============================] - 0s 95us/sample - loss: 15.1679 - mae: 2.8578 - val_loss: 28.6839 - val_mae: 3.9814\n",
      "Epoch 262/300\n",
      "270/270 [==============================] - 0s 115us/sample - loss: 17.1626 - mae: 3.0639 - val_loss: 30.3394 - val_mae: 4.1591\n",
      "Epoch 263/300\n",
      "270/270 [==============================] - 0s 111us/sample - loss: 17.3634 - mae: 3.1440 - val_loss: 29.9825 - val_mae: 3.9669\n",
      "Epoch 264/300\n",
      "270/270 [==============================] - 0s 103us/sample - loss: 13.4982 - mae: 2.6713 - val_loss: 32.4519 - val_mae: 3.8221\n",
      "Epoch 265/300\n",
      "270/270 [==============================] - 0s 100us/sample - loss: 13.1339 - mae: 2.5960 - val_loss: 29.7147 - val_mae: 3.8465\n",
      "Epoch 266/300\n",
      "270/270 [==============================] - 0s 100us/sample - loss: 14.3085 - mae: 2.7241 - val_loss: 34.0522 - val_mae: 4.2464\n",
      "Epoch 267/300\n",
      "270/270 [==============================] - 0s 111us/sample - loss: 13.4561 - mae: 2.6733 - val_loss: 32.7926 - val_mae: 3.7962\n",
      "Epoch 268/300\n",
      "270/270 [==============================] - 0s 104us/sample - loss: 13.4388 - mae: 2.7687 - val_loss: 34.2781 - val_mae: 4.0600\n",
      "Epoch 269/300\n",
      "270/270 [==============================] - 0s 100us/sample - loss: 13.8144 - mae: 2.6685 - val_loss: 32.4897 - val_mae: 3.8209\n",
      "Epoch 270/300\n",
      "270/270 [==============================] - 0s 100us/sample - loss: 13.3292 - mae: 2.6515 - val_loss: 37.2993 - val_mae: 4.0985\n",
      "Epoch 271/300\n",
      "270/270 [==============================] - 0s 89us/sample - loss: 16.1341 - mae: 2.9304 - val_loss: 51.1189 - val_mae: 5.1133\n",
      "Epoch 272/300\n",
      "270/270 [==============================] - 0s 103us/sample - loss: 16.6428 - mae: 3.1103 - val_loss: 37.9392 - val_mae: 4.1799\n",
      "Epoch 273/300\n",
      "270/270 [==============================] - 0s 92us/sample - loss: 14.4184 - mae: 2.7192 - val_loss: 38.8408 - val_mae: 4.1765\n",
      "Epoch 274/300\n",
      "270/270 [==============================] - 0s 92us/sample - loss: 14.1075 - mae: 2.7486 - val_loss: 31.1283 - val_mae: 3.8211\n",
      "Epoch 275/300\n",
      "270/270 [==============================] - 0s 96us/sample - loss: 15.7017 - mae: 2.9697 - val_loss: 33.0207 - val_mae: 3.8962\n",
      "Epoch 276/300\n",
      "270/270 [==============================] - 0s 100us/sample - loss: 18.0375 - mae: 3.1485 - val_loss: 30.5592 - val_mae: 4.3052\n",
      "Epoch 277/300\n",
      "270/270 [==============================] - 0s 103us/sample - loss: 15.7675 - mae: 2.8673 - val_loss: 28.8753 - val_mae: 4.0233\n",
      "Epoch 278/300\n",
      "270/270 [==============================] - 0s 104us/sample - loss: 13.5061 - mae: 2.6743 - val_loss: 32.7086 - val_mae: 4.5436\n",
      "Epoch 279/300\n",
      "270/270 [==============================] - 0s 100us/sample - loss: 20.0394 - mae: 3.3915 - val_loss: 31.8645 - val_mae: 4.6804\n",
      "Epoch 280/300\n",
      "270/270 [==============================] - 0s 96us/sample - loss: 17.3821 - mae: 3.1626 - val_loss: 29.9377 - val_mae: 4.1580\n",
      "Epoch 281/300\n",
      "270/270 [==============================] - 0s 103us/sample - loss: 15.3929 - mae: 2.8863 - val_loss: 34.1630 - val_mae: 4.0892\n",
      "Epoch 282/300\n",
      "270/270 [==============================] - 0s 96us/sample - loss: 14.6381 - mae: 2.9466 - val_loss: 31.5025 - val_mae: 3.7140\n",
      "Epoch 283/300\n",
      "270/270 [==============================] - 0s 100us/sample - loss: 14.1051 - mae: 2.6540 - val_loss: 32.3912 - val_mae: 4.1318\n",
      "Epoch 284/300\n",
      "270/270 [==============================] - 0s 102us/sample - loss: 13.7409 - mae: 2.7304 - val_loss: 28.3767 - val_mae: 3.8803\n",
      "Epoch 285/300\n",
      "270/270 [==============================] - 0s 100us/sample - loss: 13.4430 - mae: 2.6942 - val_loss: 29.3701 - val_mae: 3.7569\n",
      "Epoch 286/300\n",
      "270/270 [==============================] - 0s 96us/sample - loss: 12.8104 - mae: 2.5761 - val_loss: 31.5413 - val_mae: 3.9024\n",
      "Epoch 287/300\n",
      "270/270 [==============================] - 0s 92us/sample - loss: 12.8204 - mae: 2.5508 - val_loss: 29.2064 - val_mae: 4.1069\n",
      "Epoch 288/300\n",
      "270/270 [==============================] - 0s 104us/sample - loss: 13.5483 - mae: 2.6923 - val_loss: 28.3564 - val_mae: 4.0751\n",
      "Epoch 289/300\n",
      "270/270 [==============================] - 0s 92us/sample - loss: 13.2407 - mae: 2.6908 - val_loss: 29.3181 - val_mae: 3.8241\n",
      "Epoch 290/300\n",
      "270/270 [==============================] - 0s 101us/sample - loss: 13.3267 - mae: 2.6265 - val_loss: 33.2107 - val_mae: 4.7753\n",
      "Epoch 291/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270/270 [==============================] - 0s 103us/sample - loss: 16.6652 - mae: 3.1242 - val_loss: 29.5140 - val_mae: 4.4196\n",
      "Epoch 292/300\n",
      "270/270 [==============================] - 0s 102us/sample - loss: 15.7241 - mae: 2.9529 - val_loss: 30.4105 - val_mae: 4.3490\n",
      "Epoch 293/300\n",
      "270/270 [==============================] - 0s 111us/sample - loss: 14.1592 - mae: 2.7263 - val_loss: 28.8774 - val_mae: 3.8647\n",
      "Epoch 294/300\n",
      "270/270 [==============================] - 0s 110us/sample - loss: 13.1256 - mae: 2.6176 - val_loss: 28.8321 - val_mae: 3.8560\n",
      "Epoch 295/300\n",
      "270/270 [==============================] - 0s 103us/sample - loss: 13.2794 - mae: 2.6419 - val_loss: 32.2571 - val_mae: 4.0315\n",
      "Epoch 296/300\n",
      "270/270 [==============================] - 0s 92us/sample - loss: 14.5318 - mae: 2.7793 - val_loss: 28.0512 - val_mae: 3.9408\n",
      "Epoch 297/300\n",
      "270/270 [==============================] - 0s 97us/sample - loss: 15.6800 - mae: 2.9182 - val_loss: 28.7115 - val_mae: 3.8618\n",
      "Epoch 298/300\n",
      "270/270 [==============================] - 0s 105us/sample - loss: 14.0608 - mae: 2.6735 - val_loss: 28.8481 - val_mae: 3.8719\n",
      "Epoch 299/300\n",
      "270/270 [==============================] - 0s 100us/sample - loss: 12.9316 - mae: 2.6194 - val_loss: 34.0179 - val_mae: 4.0725\n",
      "Epoch 300/300\n",
      "270/270 [==============================] - 0s 92us/sample - loss: 13.7447 - mae: 2.7101 - val_loss: 29.7496 - val_mae: 3.6670\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets.boston_housing import load_data\n",
    "\n",
    "# 데이터를 다운받습니다.\n",
    "(x_train, y_train), (x_test, y_test) = load_data(path='boston_housing.npz',\n",
    "                                                 test_split=0.2,\n",
    "                                                 seed=777)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# 데이터 표준화\n",
    "# 전처리를 진행하지 않습니다.\n",
    "# mean = np.mean(x_train, axis = 0)\n",
    "# std = np.std(x_train, axis = 0)\n",
    "\n",
    "# x_train = (x_train - mean) / std\n",
    "# x_test = (x_test - mean) / std\n",
    "\n",
    "# 검증 데이터셋을 만듭니다.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, \n",
    "                                                  test_size = 0.33, \n",
    "                                                  random_state = 777)\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "# 입력 데이터의 형태를 꼭 명시해야 합니다.\n",
    "# 13차원의 데이터를 입력으로 받고, 64개의 출력을 가지는 첫 번째 Dense 층\n",
    "model.add(Dense(64, activation = 'relu', input_shape = (13, )))\n",
    "model.add(Dense(32, activation = 'relu')) # 32개의 출력을 가지는 Dense 층\n",
    "model.add(Dense(1)) # 하나의 값을 출력합니다.\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'mse', metrics = ['mae'])\n",
    "\n",
    "# 전처리한 코드의 결과와 비교했을때, 매우 큰 차이가 남을 볼 수 있습니다.\n",
    "# 또한, 학습이 진행되고 있는지 loss 값과 metrics를 확인하면서 점검해보세요.\n",
    "# 이번 문제를 통해 전처리 작업의 중요성(여기서는 스케일링)을 알 수 있습니다.\n",
    "history = model.fit(x_train, y_train, \n",
    "                    epochs = 300, \n",
    "                    validation_data = (x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "work"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
